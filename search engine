import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

class WebCrawler:
    def __init__(self, starting_url, max_depth=2):
        self.starting_url = starting_url
        self.max_depth = max_depth
        self.visited_urls = set()

    def crawl(self, url, depth=0):
        if depth > self.max_depth or url in self.visited_urls:
            return

        self.visited_urls.add(url)
        try:
            response = requests.get(url)
            if response.status_code == 200:
                content = response.text
                yield url, content

                soup = BeautifulSoup(content, "html.parser")
                for link in soup.find_all("a", href=True):
                    next_url = urljoin(url, link["href"])
                    yield from self.crawl(next_url, depth + 1)
        except Exception as e:
            print(f"Error crawling {url}: {e}")

class SimpleSearchEngine:
    def __init__(self):
        self.documents = {}
        self.indexed_urls = set()

    def add_document(self, document_id, content):
        self.documents[document_id] = content.lower()

    def crawl_and_index(self, starting_url, max_depth=2):
        crawler = WebCrawler(starting_url, max_depth)
        for url, content in crawler.crawl(starting_url):
            doc_id = len(self.documents) + 1
            self.add_document(doc_id, content)
            self.indexed_urls.add(url)

    def search(self, query):
        query = query.lower()
        results = []

        for doc_id, content in self.documents.items():
            if query in content:
                results.append(doc_id)

        return results

if __name__ == "__main__":
    search_engine = SimpleSearchEngine()

    # Add documents to the search engine
    search_engine.add_document(1, "Python is a programming language.")
    search_engine.add_document(2, "Java is another popular language.")
    search_engine.add_document(3, "Python and Java are widely used in software development.")

    # Crawl the web and index content
    starting_url = "https://example.com"  # Replace with your starting URL
    search_engine.crawl_and_index(starting_url, max_depth=2)

    # Perform a search
    query = "Python"
    search_results = search_engine.search(query)

    if search_results:
        print(f"Search results for '{query}':")
        for result in search_results:
            print(f"Document ID: {result}")
    else:
        print("No results found.")

